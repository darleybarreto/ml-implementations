{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliotecas extras utilizadas\n",
    "#### nltk\n",
    "para a instalação do RSLPStemmer, punkt e stopwords, precisa-se fazer \"import nltk\" e \"nltk.download_shell()\" no terminal, então escolher a opção \"d\" e digitar \"rslp\" e depois o mesmo procedimento para \"stopwords\" e \"punkt\"\n",
    "#### lxml\n",
    "#### pandas\n",
    "#### seaborn\n",
    "#### matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import (TfidfTransformer,\n",
    "                                             CountVectorizer)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.model_selection import (cross_val_score,\n",
    "                                      KFold)\n",
    "\n",
    "from sklearn.linear_model import (SGDClassifier,\n",
    "                                  RidgeClassifier,\n",
    "                                  Perceptron,\n",
    "                                  PassiveAggressiveClassifier)\n",
    "\n",
    "from sklearn.neighbors import (KNeighborsClassifier,\n",
    "                               NearestCentroid)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.naive_bayes import (BernoulliNB,\n",
    "                                 MultinomialNB)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from nltk.stem import RSLPStemmer\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.data import load\n",
    "\n",
    "from nltk.corpus import stopwords as stpw\n",
    "\n",
    "from numpy import (zeros,\n",
    "                   array)\n",
    "\n",
    "from re import sub\n",
    "from time import time\n",
    "from lxml import etree as ET\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean(dirty_file):\n",
    "    for i in range(14):\n",
    "        dirty_file.readline()\n",
    "\n",
    "    xml_file = \"<items>\\n\" + \"\\n\".join(dirty_file.readlines()) + \"</items>\\n\"\n",
    "\n",
    "    parser = ET.XMLParser(recover=True)\n",
    "    tree = ET.fromstring(xml_file, parser=parser)\n",
    "    lista = []\n",
    "    attri = set()\n",
    "    \n",
    "    for i in tree.getchildren():\n",
    "        attri.add(i.attrib['category'])\n",
    "        texto = [i.text for i in i.getchildren() if i.tag == 'text']\n",
    "        if texto[0]:\n",
    "            lista.append({i.attrib['category']:texto[0]})\n",
    "\n",
    "    return {key: value for key, value in zip(attri, range(len(attri)))}, lista\n",
    "\n",
    "\n",
    "def transform(ldocs, stop, stemmer, sent_tokenizer):\n",
    "    \"\"\"\n",
    "    This function receive a python list of texts, a python list of stopwords and a nltk stemmer object.\n",
    "    It returns two numpy arrays, the first is an array of stemmed texts and no stopwrods,\n",
    "    the second is an array with the labels of every single element from the first array.\n",
    "\n",
    "    path_txt -> path to folder txt files\n",
    "    ldocs -> python list of file names\n",
    "    stop -> list with all stop words\n",
    "    stemmer -> a stemmer object from ntlk, like SnowballStemmer(\"english\")\n",
    "    label -> boolean indicator of classification(classficador.py) or train(treinamento.py)\n",
    "    \"\"\"\n",
    "    \n",
    "    docs_cleared = []\n",
    "    labels = []\n",
    "    \n",
    "    for doc in ldocs:\n",
    "        with open(doc, \"r\") as dirty_file:\n",
    "            \n",
    "            classes, file = clean(dirty_file)\n",
    "            \n",
    "            clean_definitions = [sub('[^\\w\\s-]','',\" \".join([stemmer.stem(word) for word in list(filter(lambda x: x.lower() not in stop, [i  for i in sum([word_tokenize(sent, language='portuguese') for sent in sent_tokenizer.tokenize(list(definition.values())[0])],[]) if i not in ',.-\"!;:?']))])) for definition in file]\n",
    "            \"\"\"\n",
    "            desmembrando o comando acima, temos:\n",
    "            for definition in file\n",
    "                list(definition.values())[0] -> pega cada deficinao/texto da noticia\n",
    "                assim, realiza o seguinte comando\n",
    "                sent_tokenizer.tokenize(list(definition.values())[0]) que gera uma lista de tokens por setenca (sent)\n",
    "                para cada texto da noticia, entao a chamada word_tokenize(sent, language='portuguese') retorna uma lista\n",
    "                de tokens de cada palavra de cada sentenca, sendo assim, temos uma lista de listas, entao realizamos\n",
    "                o flatten dessa lista para retornar apenas uma, com o comando sum(minhaLista,[]), o resultado\n",
    "                do flatten eh entao passado por um filter, onde tira-se as stopwords e por fim realiza-se o stemming de\n",
    "                cada palavra da lista, posteriormente fazemos o join das palavras e realizando um processamento\n",
    "                de regex para retornar o texto da noticia perfeitamente tratado\n",
    "            \"\"\"\n",
    "            labels.append([classes[list(d.keys())[0]] for d in file])\n",
    "\n",
    "            docs_cleared.append(clean_definitions)\n",
    "           \n",
    "    return array(sum(docs_cleared, [])), array(sum(labels, []))\n",
    "\n",
    "def evaluate(docs, labels, classifier, k_fold):\n",
    "    \"\"\"\n",
    "    Function to performe the train/test\n",
    "\n",
    "    docs -> python list with texts from documents\n",
    "    labels -> labels from definitions in docs\n",
    "    classifier -> scikit classfier\n",
    "    k_fold -> a k fold object from scikit\n",
    "    \"\"\"\n",
    "    clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('classifier', classifier)])\n",
    "        \n",
    "    scores = []\n",
    "    classes = list(set(labels))\n",
    "    start = time()\n",
    "    for train_indices, test_indices in k_fold.split(docs):\n",
    "        \n",
    "        train_text = docs[train_indices]\n",
    "        train_y = labels[train_indices]\n",
    "\n",
    "        test_text = docs[test_indices]\n",
    "        test_y = labels[test_indices]\n",
    "        \n",
    "        clf.fit(train_text, train_y)\n",
    "        predictions = clf.predict(test_text)\n",
    "        \n",
    "        score = accuracy_score(test_y, predictions)\n",
    "        scores.append(score)\n",
    "    end = time()\n",
    "    return len(docs), sum(scores)/len(scores), clf, end - start\n",
    "\n",
    "def present_info(total, score, pipe, t):\n",
    "    \"\"\"\n",
    "    Function to present some information to the user.\n",
    "\n",
    "    total -> total number of definitions of documents\n",
    "    score -> a total score from a train/test process\n",
    "    pipe -> a pipeline object from scikit\n",
    "    t -> time spent with trai/test   \n",
    "    \"\"\"\n",
    "    cls = str(pipe.get_params()[\"classifier\"].__class__.__name__)\n",
    "    \n",
    "    print(\"Using the classifier: \", cls)\n",
    "    print('Total classified:', total)\n",
    "    print('Score:', score)\n",
    "    print(\"Time spent:\", t,\"s\",\"\\n\")\n",
    "\n",
    "    return cls, score\n",
    "\n",
    "def potting(VectorizerList):\n",
    "    \n",
    "    sns.set(style=\"whitegrid\", color_codes=True)\n",
    "    TFIDF = pd.DataFrame({'Classificadores': [i for i,_ in VectorizerList], 'Scores': [j for _,j in VectorizerList]})\n",
    "    x_tfidf = array(range(len(VectorizerList)))\n",
    "    plt.xticks(x_tfidf,[i for i,_ in VectorizerList], rotation=30)\n",
    "    plt.plot(x_tfidf, [j for _,j, in VectorizerList], marker='o', linestyle='--')\n",
    "    plt.grid(True)\n",
    "    plot = sns.pointplot(palette=\"Set2\", dodge=True, markers=[\"x\"], x='Classificadores',y='Scores',data=TFIDF).set_xticklabels(TFIDF['Classificadores'], rotation=35)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ldocs = ['news_data.xml']\n",
    "VectorizerList = []\n",
    "stemmer = RSLPStemmer()\n",
    "stopwords = stpw.words('portuguese')\n",
    "tokenizer = load('tokenizers/punkt/portuguese.pickle')\n",
    "\n",
    "algorithms= {'LinearSVC' : LinearSVC(),\\\n",
    "            'MultinomialNB' : MultinomialNB(alpha=0.01),\\\n",
    "            'SGDClassifier' : SGDClassifier(n_iter=500),\\\n",
    "            'RidgeClassifier' : RidgeClassifier(tol=1e-2, solver=\"sag\"),\\\n",
    "            'Perceptron' : Perceptron(n_iter=300),\\\n",
    "            'PassiveAggressiveClassifier' : PassiveAggressiveClassifier(n_iter=300),\\\n",
    "            'KNeighborsClassifier' : KNeighborsClassifier(n_neighbors=50),\\\n",
    "            'RandomForestClassifier' : RandomForestClassifier(n_estimators=30),\\\n",
    "            'BernoulliNB' : BernoulliNB(alpha=.01),\\\n",
    "            'DecisionTreeClassifier' : DecisionTreeClassifier(random_state=0),\\\n",
    "            'NearestCentroid' : NearestCentroid()}\n",
    "\n",
    "k_fold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "docs, labels = transform(ldocs, stopwords, stemmer, tokenizer)\n",
    "\n",
    "i = time()\n",
    "for clf in algorithms.values():\n",
    "    VectorizerList.append(present_info(*evaluate(docs, labels, clf, k_fold)))\n",
    "f = time()\n",
    "print(\"Classification took {} second(s)\".format(f-i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abaixo temos a comparação entre os classificadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "potting(VectorizerList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

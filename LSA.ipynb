{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from json import loads\n",
    "from math import log\n",
    "from re import sub\n",
    "from nltk.stem import SnowballStemmer\n",
    "from scipy.linalg import (diagsvd,\n",
    "                          svd)\n",
    "from numpy import (zeros,\n",
    "                   asarray, \n",
    "                   sum, \n",
    "                   dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LSA(object):\n",
    "    \n",
    "\tdef __init__(self, stopwords, ldocs, language):\n",
    "        \n",
    "\t\tself.docs = [self._read_file(file) for file in ldocs]\n",
    "\t\tself.stopwords = stopwords\n",
    "\t\tself.language = language  \n",
    "\t\tself.wdict = {}\n",
    "\t\tself.keys = None\n",
    "\t\tself._A = None\n",
    "\t\tself.dcount = 0\n",
    "        \n",
    "\tdef _read_file(self,file_name):\n",
    "\t\twith open(file_name, \"r\") as file:\n",
    "\t\t\treturn [line.rstrip('\\n')  for line in file]\n",
    "\n",
    "        \n",
    "\tdef _parse(self):\n",
    "\t\t\"\"\"\n",
    "\t\t@docs eh uma list de documentos, onde cada posicao contem listas de definicoes\n",
    "\t\t@lang eh o idioma dos documentos\n",
    "\t\tRecebe uma lista de documentos, para cada documento, quebra em palavras, remove os caracteres a serem ignorados e \n",
    "\t\ttransforma tudo em letras minusculas, assim as palavras podem ser comparadas com as \n",
    "\t\tstop words. Se a palavra eh uma stop words, ela eh ignorada e passa para a proxima palavra. \n",
    "\t\tSe nao for uma stop words, colocamos a palavra no dict, e tambem acrescentamos o numero do documento atual \n",
    "\t\tpara manter o controle de quais os documentos que a palavra aparece.\n",
    "\n",
    "\n",
    "\t\tOs documentos em que cada palavra aparece sao mantidos em uma lista associada a essa palavra no dics. \n",
    "\t\tPor exemplo, uma vez que a palavra 'livro' aparece em titulos 3 e 4, terÃ­amos self.wdict['livro'] = [3, 4] \n",
    "\t\tapÃ³s todos os titulos serem analisados.\n",
    "\t\t\"\"\"\n",
    "\t\tldocs, self.docs = self.docs, list()\n",
    "\n",
    "\t\tsno = SnowballStemmer(self.language)\n",
    "            \n",
    "\t\t#a linha abaixo faz com que cada elemento da lista docs agora eh uma area com n definicoes de termos diferentes\n",
    "\t\tself.docs = [\" \".join(doc) for doc in self.docs]\n",
    "\t\tfor doc in self.docs:\n",
    "\n",
    "\t\t\tstrdoc = sub('[^\\w\\s-]','', doc).lower()\n",
    "\t\t\twords = list(filter(lambda x: x not in self.stopwords, strdoc.split()))\n",
    "        \n",
    "\t\t\tfor w in words:\n",
    "\t\t\t\tw = sno.stem(w)\n",
    "\t\t\t\tif w in self.wdict:\n",
    "\t\t\t\t\tself.wdict[w].append(self.dcount)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tself.wdict[w] = [self.dcount]\n",
    "\t\t\tself.dcount += 1\n",
    "\n",
    "\tdef _run_matrix(self):\n",
    "\t\tfor i, k in enumerate(self.keys):\n",
    "\t\t\tfor d in self.wdict[k]:\n",
    "\t\t\t\tself._A[i,d] += 1\n",
    "\n",
    "\tdef _build_matrix(self):\n",
    "\t\t\"\"\"\n",
    "\t\tUma vez que todos os documentos forem analisados, todas as palavras (chaves do dict) que estao \n",
    "\t\tem mais do que um documento sao extraidas e classificadas, e a matriz eh construida com o numero de linhas\n",
    "\t\tigual ao numero de palavras (chaves), e o numero de colunas igual a contagem de documentos. \n",
    "\t\tFinalmente, para cada par [palavra (chave), documento] da celula da matriz correspondente Ã© incrementado.\n",
    "\t\t\"\"\"\n",
    "\t\tself.keys = [k for k in self.wdict if len(self.wdict[k]) > 1] \n",
    "\t\tself.keys.sort() \n",
    "\t\tself._A = zeros([len(self.keys), self.dcount])\n",
    "\t\tself._run_matrix()\n",
    "\n",
    "\tdef _TFIDF(self):\n",
    "\t\t\"\"\"\n",
    "\t\tself._A[i,j] eh a frequencia da palavra i no documento j (contagem da celula)\n",
    "\t\tWordsPerDoc eh o numero maximo de ocorrencia de qualquer palavra no mesmo documento (somatorio das contagens na culuna j)\n",
    "\t\tcols eh o numero de documento na colecao (numero de colunas)\n",
    "\t\tDocsPerWord eh a quantidade de documentos em que a palavra i aparece (numero de colunas != 0 na linha i)\n",
    "\t\t\"\"\"\n",
    "\t\t#Somando as colunas da matriz A\n",
    "\t\tWordsPerDoc = sum(self._A, axis=0)\n",
    "\t\t\n",
    "\t\t#O comando self._A > 0 retorna um vetor de booleano de mesma dimensao que self._A\n",
    "\t\t#O comando asarray(self._A > 0, 'i') faz com que o vetor de booleanos vire binario {0 quando False, 1 quando True}\n",
    "\t\t#O comando sum(asarray(self._A > 0, 'i'), axis=1) faz a soma com base no axis = 1, isto eh, soma as linhas da matriz, logo\n",
    "\t\t#o retorno deste comando sera uma matriz unidimensional contendo o somatorio de todas as linhas(palavras/chaves) da matriz A\n",
    "\t\tDocsPerWord = sum(asarray(self._A > 0, 'i'), axis=1)\n",
    "\t\trows, cols = self._A.shape\n",
    "        \n",
    "\t\t#with errstate(invalid='ignore'):\n",
    "\t\tfor i in range(rows):\n",
    "\t\t\tfor j in range(cols):\n",
    "\t\t\t\tnumerator = 0\n",
    "\t\t\t\tif WordsPerDoc[j] != 0.0:\n",
    "\t\t\t\t\tnumerator = self._A[i,j] / WordsPerDoc[j]\n",
    "\t\t\t\tself._A[i,j] = numerator * (log(float(cols) / DocsPerWord[i]))\n",
    "\n",
    "\tdef calculate(self):\n",
    "\t\t\"\"\"\n",
    "\t\tAqui iremos fazer a reducao da dimensao atraves do SVD.\n",
    "\t\tO SVD eh util pois ele encontra a representacao dimensional reduzida da matriz que enfatiza as relacoes mais fortes\n",
    "\t\te joga fora o ruido. Em outras palavras, ele faz a melhor reconstrucao da matriz possivel com o minimo de informacao.\n",
    "\t\tO truque de usar o SVD eh descobrir quantas dimensoes ou \"conceitos\" usar para aproximar a matriz.\n",
    "\t\tPoucas dimensoes e padroes importantes sao deixados fora.\n",
    "\n",
    "\n",
    "\t\tA fim de escolher o numero certo de dimensoes para usar, podemos fazer um histograma do quadrado de valores unicos. \n",
    "\t\tIsto representa graficamente a importancia da contribuicao de cada valor para aproximar nossa matriz.\n",
    "\n",
    "\t\tNo caso de grandes colecoes de documentos, o numero de dimensÃµes utilizadas esta no intervalo de 100 a 500.\n",
    "\t\t\"\"\"\n",
    "\t\trows, cols = self._A.shape\n",
    "\n",
    "\t\tself.U, self.S, self.Vt = svd(self._A, full_matrices=False)\n",
    "\n",
    "\t\t#Reconstruindo a matriz, pela a aproximada matriz'\n",
    "\t\ttransformed_matrix = dot(dot(self.U, diagsvd(self.S, len(self.S), len(self.Vt))),self.Vt)\n",
    "        \n",
    "\t\treturn transformed_matrix\n",
    "        \n",
    "\tdef run(self):\n",
    "\t\tself._parse()\n",
    "\t\tself._build_matrix()\n",
    "\t\tself._TFIDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    with open(\"stopwords.json\",\"r\") as file:\n",
    "        stop = loads(file.read());\n",
    "\n",
    "    ldocs = []\n",
    "    \n",
    "    lsa = LSA(stop, ldocs, \"english\")\n",
    "\n",
    "    lsa.run()\n",
    "\n",
    "    transformated = lsa.calculate()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
